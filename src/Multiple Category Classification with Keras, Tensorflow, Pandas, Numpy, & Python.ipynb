{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multiple Category Classification with Keras, Tensorflow, Pandas, Numpy, & Python](https://static.codingforentrepreneurs.com/media/cfe-blog/multiple-categeory-classification-keras-tensorflow/Multi-Category-Classification-Python.jpg)\n",
    "Original Post is [here](https://www.codingforentrepreneurs.com/blog/multiple-categeory-classification-keras-tensorflow)\n",
    "\n",
    "In this one, we'll be creating a deep neural network since it will help us do is find patterns in a large amount of data to make the best possible prediction on new data. Before we get started, let's revisit an old adage:\n",
    "\n",
    "> Good data in, good data out. Garbage in, garbage out.\n",
    "\n",
    "Building a neural network is easy even if you're new to writing code. The part that's hard is getting the data ready, setting the correct parameters, and understanding the math behind the neural network. The thing is, we don't have to understand the math to actually use a neural network.\n",
    "\n",
    "In this post, we'll do 4 things:\n",
    "\n",
    "1. Pre-process pre-existing data with built in tools\n",
    "2. Run a neural network with Keras and Tensorflow\n",
    "3. Plot neural network performance\n",
    "4. Save trained model for continued-training and predictions (inference)\n",
    "\n",
    "\n",
    "Thanks to [Peter Nagy](https://github.com/nagypeterjob) for the HUGE inspiration for creating this notebook.\n",
    "\n",
    "### Jupyter Notebook is [here](https://github.com/codingforentrepreneurs/Notebooks/blob/master/src/API%20Calls%20within%20a%20Pandas%20Dataframe%20API%20using%20Pandas%20Apply.ipynb)\n",
    "The jupyter notebook is an amazing way to run live python code for data science and deep learning, it's also how this post is formatted.  Learn how to create your own [jupyter notebook server here](https://www.codingforentrepreneurs.com/blog/jupyter-notebook-server-aws-ec2-aws-vpc).\n",
    "\n",
    "\n",
    "### Installation Requirements:\n",
    "\n",
    "```\n",
    "pip install Keras==2.2.4 pandas>=0.25.0 numpy<17.0 sklearn tensorflow==1.14.0\n",
    "```\n",
    "> Simply run `!pip install ...` within a cell to install within a jupyter notebook.\n",
    "\n",
    "Full notebook requirements are located: `../requirements/multiple_category_classification_keras.txt`\n",
    "\n",
    "\n",
    "### Recommend Hardware Setup & Guides:\n",
    "- **Nvidia GPU with Cuda/CuDNN** GPUs are critical in processing large matrix operations; that's essentially what's happening in a neural network. We have gaming to thank for the huge advancements in GPUs.\n",
    "    - [Install Tensorflow GPU on Windows using CUDA and cuDNN](https://www.codingforentrepreneurs.com/blog/install-tensorflow-gpu-windows-cuda-cudnn)\n",
    "- **Jupyter Notebook** Using jupyter makes your life much easier as your working out your code especially as it relates to data science and visualizing what you're working on. \n",
    "    - A [Jupyter Notebook Server](https://www.codingforentrepreneurs.com/blog/jupyter-notebook-server-aws-ec2-aws-vpc) can be very useful so then you don't have to invest in your own local hardware (like the guide above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Keras==2.2.4 pandas>=0.25.0 numpy<17.0 sklearn tensorflow==1.14.0 matplotlib==3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# disable tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = os.getcwd()\n",
    "BASE_DIR = os.path.dirname(CURRENT_DIR)\n",
    "data_dir = os.path.join(BASE_DIR, 'data')\n",
    "models_dir = os.path.join(BASE_DIR, 'neural_networks', 'models')\n",
    "checkpoints_dir = os.path.join(BASE_DIR, 'neural_networks', 'checkpoints')\n",
    "pickles_dir = os.path.join(BASE_DIR, 'neural_networks', 'pickles')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(pickles_dir, exist_ok=True)\n",
    "\n",
    "dataset_path = os.path.join(data_dir, 'uci-news-aggregator.csv')\n",
    "DEFAULT_MODEL_NAME = 'multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name=DEFAULT_MODEL_NAME):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "        patience=7, \n",
    "        min_delta=0.0001)\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, name)\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    filepath = os.path.join(\n",
    "        checkpoint_path,\n",
    "        'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "    )\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "        monitor='val_loss',  \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    callbacks = [early_stopping, checkpoint]\n",
    "    return callbacks\n",
    "\n",
    "def save_model(model, name=DEFAULT_MODEL_NAME):\n",
    "    filename = os.path.join(models_dir, name + '.hdf5')\n",
    "    return model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path, usecols=['TITLE', 'CATEGORY'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling Duplicates\n",
    "As a part of pre-processing data, we want to remove duplicate data as much as possible. Let's take a look to see if we have any title duplicates within our dataset. We use `df.TITLE` because `TITLE` is the actual name of the column. `df.CATEGORY` is the other column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_title_distribution = df.TITLE.value_counts()[:10]\n",
    "duplicate_title_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_title = duplicate_title_distribution.index[0]\n",
    "most_common_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['TITLE'].str.contains(most_common_title)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the most common article title: \n",
    "\n",
    "`The article requested cannot be found! Please refresh your browser or go back  ...` \n",
    "\n",
    "That occurred 145 times! If you're familiar with web scraping, you'll know that this is probably a 404 page error but it's clearly in our data and causing issues. Let's just remove *all duplicate titles* to ensure we don't have data like `PR Newswire` as being one of our possible data points.\n",
    "\n",
    "Basically, my thought is, if the article title is duplicated, it's probably not a good article title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='TITLE', keep=False)\n",
    "df[df['TITLE'].str.contains(most_common_title)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the distribution of each article title and it's respective category. We're looking for the category with the **least** number of titles associated to it. We want an even distribution of titles / categories for best results otherwise we should anticipate or results to be skewed incorrectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea29d595-26b0-4d83-a005-853fa59f4506",
    "_uuid": "acf2450933eb3586930df738829abd2e11646e14"
   },
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    'e': 'entertainment', \n",
    "    'b':'business', \n",
    "    't': 'science/tech', \n",
    "    'm': 'health'\n",
    "}\n",
    "df.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {\n",
    "    'e': 'entertainment', \n",
    "    'b': 'business', \n",
    "    't': 'science/tech', \n",
    "    'h': 'health'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "44de46e2-acce-470d-9c46-624cb0dd15b9",
    "_uuid": "eb9f4766b60f5a23901a8bde1d901ced6c7a3b3e"
   },
   "outputs": [],
   "source": [
    "max_num_of_labels = df.CATEGORY.value_counts()[-1] # based on the least_number of categories in the value counts above.\n",
    "\n",
    "data_df = df.copy() # I create copies of the input data to ensure I always have the original copy readily available.\n",
    "\n",
    "shuffled_df = data_df.reindex(np.random.permutation(data_df.index)) # always shuffle data when you can.\n",
    "\n",
    "\n",
    "e = shuffled_df[shuffled_df['CATEGORY'] == 'e'][:max_num_of_labels]\n",
    "b = shuffled_df[shuffled_df['CATEGORY'] == 'b'][:max_num_of_labels]\n",
    "t = shuffled_df[shuffled_df['CATEGORY'] == 't'][:max_num_of_labels]\n",
    "m = shuffled_df[shuffled_df['CATEGORY'] == 'm'][:max_num_of_labels]\n",
    "\n",
    "concated_df = pd.concat([e,b,t,m], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated_df = concated_df.reindex(np.random.permutation(concated_df.index))\n",
    "concated_df['LABEL'] = 0\n",
    "concated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d3da0fd-6d73-4f3b-b06b-d2bba34bbd4a",
    "_uuid": "60febe37826f220106adf69a51dad124cfae45cc"
   },
   "outputs": [],
   "source": [
    "#One-hot encode the label\n",
    "concated_df.loc[concated_df['CATEGORY'] == 'e', 'LABEL'] = 0 # e = index 0\n",
    "concated_df.loc[concated_df['CATEGORY'] == 'b', 'LABEL'] = 1 # b = index 1\n",
    "concated_df.loc[concated_df['CATEGORY'] == 't', 'LABEL'] = 2 # t = index 2\n",
    "concated_df.loc[concated_df['CATEGORY'] == 'm', 'LABEL'] = 3 # m = index 3\n",
    "print(concated_df['LABEL'][:10])\n",
    "labels = to_categorical(concated_df['LABEL'], num_classes=4)\n",
    "print(labels[:10])\n",
    "if 'CATEGORY' in concated_df.keys():\n",
    "    concated_df = concated_df.drop(['CATEGORY'], axis=1)\n",
    "'''\n",
    " [1. 0. 0. 0.] e\n",
    " [0. 1. 0. 0.] b\n",
    " [0. 0. 1. 0.] t\n",
    " [0. 0. 0. 1.] m\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_words = 8000\n",
    "max_len = 130\n",
    "token_filter = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words, filters=token_filter, lower=True)\n",
    "tokenizer.fit_on_texts(concated_df['TITLE'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40e6281c-2588-4ad0-991c-3d8d40791254",
    "_uuid": "0aa67be64bd63cf4350ce3f62d42c687b3143088"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(concated_df['TITLE'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2ca496ca-4bb7-40de-bf69-d86b521af51f",
    "_uuid": "97226bf26ef141c228a1123e125ef7966612db47"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4e5bcf7a-4c6b-44fc-963d-415b9338abe4",
    "_uuid": "28940e621602cfd9645a88dd43427b2431c75b5b"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "emb_dim = 128\n",
    "batch_size = 256\n",
    "labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(name='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Neural Network\n",
    "Keras makes it easy to create neural networks on top of the tensorflow library. You don't have to know exactly how this works to run it. \n",
    "\n",
    "Below will define your neural network's architecture. \n",
    "\n",
    "> [From Keras docs](https://keras.io/losses/): When using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample)...\n",
    "\n",
    "We're doing categorical classification vs binary classification. \n",
    "\n",
    "**Binary classification** means predictions will be one or the other ie, `cat vs dog` or `liked vs disliked`.\n",
    "\n",
    "**Category classification** means prediction \"category\" your new data belows to ie, `cat vs dog vs bird vs chair` or what we did here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79a42a6f-01f4-4e74-b645-321f2a0a6e39",
    "_uuid": "f50c8494777ca5141da8c23bb932e531a82b89d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run training\n",
    "This step will take a while. It will be much faster if you lower the number of epochs (above) as well as use a GPU (as mentioned at the top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa53cfb9-75f7-47ee-b53d-b7f241ee082a",
    "_uuid": "a16c336b7eae3d72c7c92cf799702eacf70677c7"
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test, y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = training.history['acc']\n",
    "val_accuracy = training.history['val_acc']\n",
    "loss = training.history['loss']\n",
    "val_loss = training.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1130440c-dd13-4f36-9657-01b13f322efb",
    "_uuid": "f8400fe47eebbb7e8456d6f3617c6bcd7eccecf6"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9af38e60-0811-485d-8da3-32a744b53365",
    "_uuid": "25ba8e0b354451cc482a93324e42781fde6d0826"
   },
   "outputs": [],
   "source": [
    "txt = [\"Regular fast food eating linked to fertility issues in women\"]\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_len)\n",
    "pred = model.predict(padded)\n",
    "labels = ['entertainment', 'business', 'science/tech', 'health']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(index):\n",
    "    '''\n",
    "    The labels correspond to exact label indices, in other words, the \n",
    "    order is absolutely important.\n",
    "    '''\n",
    "    labels = ['entertainment', 'business', 'science/tech', 'health']\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model_klass=model):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    pred = model_klass.predict(padded)\n",
    "    top_prediction_index = np.argmax(pred)\n",
    "    predicted_label = extract_label(top_prediction_index)\n",
    "    predictions = pred.tolist()[0]\n",
    "    extracted_predictions = [{extract_label(i):\"%.2f%%\"%(x*100)} for i, x in enumerate(predictions)]\n",
    "    top_percent = \"%.2f%%\"% (predictions[top_prediction_index] * 100)\n",
    "    print(f\"{text}\\t\\t{top_percent} {predicted_label}\")\n",
    "    return extracted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"The startup is doing very well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"The startup company is booming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"The startup company's growth has been amazing so far.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Sales are through the roof!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Stocks are booming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"That was an incredible performance by the actors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"That was an incredible performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"The health of the company is poor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"The health of the kid is poor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Prepare for Reusable Prediction\n",
    "First, we'll save the model. Then we'll save the tokenizer with `pickle`. After that, we'll adjust our predict method to be resuable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, name='multi_category_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_category_tokenizer_pkl = os.path.join(pickles_dir, 'multi_category_tokenizer.pkl')\n",
    "multi_category_tokenizer_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_mode = 'wb'\n",
    "with open(multi_category_tokenizer_pkl, write_mode) as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Reusable Model\n",
    "Below is all we need: the trained model and the pickled tokenizer and now we can use our model at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "stored_model  = os.path.join(models_dir, 'multi_category_classification')\n",
    "model_obj = load_model(f'{stored_model}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_category_tokenizer_pkl = os.path.join(pickles_dir, 'multi_category_tokenizer.pkl')\n",
    "\n",
    "write_mode = 'rb'\n",
    "with open(multi_category_tokenizer_pkl, write_mode) as f:\n",
    "    tokenizer_obj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model_obj=None, tokenizer_obj=None):\n",
    "    assert(tokenizer_obj != None)\n",
    "    assert(model_obj != None)\n",
    "    seq = tokenizer_obj.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    pred = model_obj.predict(padded)\n",
    "    top_prediction_index = np.argmax(pred)\n",
    "    predicted_label = extract_label(top_prediction_index)\n",
    "    predictions = pred.tolist()[0]\n",
    "    extracted_predictions = [{extract_label(i):\"%.2f%%\"%(x*100)} for i, x in enumerate(predictions)]\n",
    "    top_percent = \"%.2f%%\"% (predictions[top_prediction_index] * 100)\n",
    "    print(f\"{text}\\t\\t{top_percent} {predicted_label}\")\n",
    "    return extracted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"This is working well.\", model_obj=model_obj, tokenizer_obj=tokenizer_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"The market viability is uncertain.\", model_obj=model_obj, tokenizer_obj=tokenizer_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
